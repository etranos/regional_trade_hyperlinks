---
title: 'Using the web to predict regional trade flows: data extraction, modelling, and validation.'
output: 
  flexdashboard::flex_dashboard:
    vertical_layout: fill
    orientation: columns
    social: menu
knit: (function(inputFile, encoding) {
    rmarkdown::render(inputFile, encoding = encoding, output_dir = "../")
    })
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, results = 'asis')
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())

#devtools::install_github("dmurdoch/leaflet@crosstalk4", force = T) # special edit of package for maps install first

library(tidyverse)
library(gridExtra)
library(rgdal)
library(maps)
library(rprojroot)
library(kableExtra)
#library(cowplot)
library(ggthemes)

library(leaflet)
library(rprojroot)
library(sf)
library(crosstalk)
library(DT)

library(geosphere)
library(RColorBrewer)

# This is the project path
path <- find_rstudio_root_file()
```

```{r Load_data, results='hide'}
path.data <- paste0(path, "/data_inter/test_t2.RData") #where total and LAD is located
load(path.data)
#total <- read_csv("total.csv", col_types = cols(X1 = col_skip())) # if local

Mapping_df <- total %>% 
  dplyr::select(orig, dest, id, io,hl, central.orig, central.dest, year)

# I need to repeat `find_rstudio_root_file()` otherwise it turns back to .Rmd floder
path <- find_rstudio_root_file()
shp <- readOGR(dsn = paste0(path, "/data_inter/Maps"), layer = "NUTS_RG_01M_2010_4326_LEVL_2",
               verbose=FALSE)

#shp <- readOGR(dsn = paste0(path, "/Raw_data/Maps"), layer = "NUTS_RG_01M_2010_4326_LEVL_2")

shp_UK <- subset(shp, NUTS_ID %in% Mapping_df$orig) 
centroids <- SpatialPointsDataFrame(coordinates(shp), data = as(shp, "data.frame")[c("NUTS_ID")])
#merge data for origins and destinations
Mapping_df <- merge(Mapping_df, centroids, by.x="orig", by.y="NUTS_ID") 
Mapping_df <- merge(Mapping_df, centroids, by.x="dest", by.y="NUTS_ID")
Names <- shp_UK@data[,c(2,4)]
Mapping_df <- merge(Mapping_df, Names, by.x="dest", by.y = "NUTS_ID")
Mapping_df <- merge(Mapping_df, Names, by.x="orig", by.y = "NUTS_ID")
names(Mapping_df) <- c("orig", "dest", "id_flow", "io", "hl", "central.orig", "central.dest","Year", "lng_orig", "lat_orig","lng_dest", "lat_dest", "Destination", "Origin")
Mapping_df <- Mapping_df[,c(1,2,3,4,5,6,7,8,11,12,9,10,13,14)]

#filter for flows 
Mapping_df <- Mapping_df %>%
  filter(Mapping_df$hl>149) 
```

Hyperlinks, NUTS2 regions
=====================================

### Observed hyperlink flows between NUTS2 regions {data-width=600}

```{r HL flow maps}
# create specific SP dataframe
flows <- gcIntermediate(Mapping_df[,9:10], Mapping_df[,11:12], sp = TRUE, addStartEnd = TRUE)
flows$ID <- Mapping_df$id_flow #id for label
flows$hl <- Mapping_df$hl #hl
flows$hl_log <- log(Mapping_df$hl)/9 #for pal
flows$io <- round(Mapping_df$io,1)
flows$origins <- Mapping_df$orig
flows$destinations <- Mapping_df$dest
flows$Year <- Mapping_df$Year
flows$Year_format <- as.Date(as.character(Mapping_df$Year), format = "%Y")# no commas
flows$Origin <- Mapping_df$Origin
flows$Destinaton <- Mapping_df$Destination

# create crosstalk sharedata
sd_map <- SharedData$new(flows)
sd_df <- SharedData$new(as.data.frame(flows@data), group = sd_map $groupName())

pal <- colorBin(palette = "plasma", domain=c(0,1.32), bins = 10, pretty = TRUE)

filter_slider("Year", "Year", sd_df, column=~Year_format, step=1, width = 600, timeFormat = "%Y")

filter_slider("Hyperlinks", "Hyperlinks", sd_df, column=~hl, step=50, max = 40000, width = 600)


#map
leaflet(options = leafletOptions(minZoom = 5)) %>%
  addProviderTiles("CartoDB.Positron") %>%
  setMaxBounds( lng1 = -8
                , lat1 = 48
                , lng2 = 2
                , lat2 = 61 ) %>%
  addPolylines(data = sd_map, weight = ~hl_log, color = ~pal(hl_log), label = ~ID)



```

Column {data-width=400}
-------------------------------------

```{r}
filter_select("Origin", "Origin", sd_df, ~Origin)

#datatable
datatable(sd_df, extensions="Scroller", style="bootstrap", class="compact", width="100%",rownames= FALSE,
          options=list(deferRender=TRUE, scrollY=300, scroller=TRUE,columnDefs = list(list(visible=FALSE, targets=c(0,2,4,5,7)))))
  #remove unwanted rows from view

```

<br>

Tranos, E., A. Carrascal-Incera and G. Willis (2022) Using the web to predict regional trade flows: data extraction, modelling, and validation, *Annals of the AAG*. In press. [Link.]()

<small>NUTS2 pairs with < 150 hyperlinks are excluded from the visualisations.</small> 


Predicted trade flows, LADs
=====================================  

### Trade flow predictions between Local Authority Districts {data-width=600}

```{r LAD data, echo=FALSE, warning=FALSE, results='hide'}
path <- find_rstudio_root_file()
path.data <- paste0(path, "/data_inter/test_t2.RData")
load(path.data)

#lad_prediction <- read_csv("lad_prediction.csv", col_types = cols(X1 = col_skip())) # if local

# I need to repeat `find_rstudio_root_file()` otherwise it turns back to .Rmd floder
path <- find_rstudio_root_file()
path.predictions <- paste0(path, "/data_inter/lad_prediction.csv")
lad_prediction <- read_csv(path.predictions, col_types = cols(X1 = col_skip()))

Lad_df <- lad_prediction %>% 
  dplyr::select(orig, dest, id, model.all2009, hl, year, lad.orig, lad.dest)

#shapefile and centroids
shp_LA <- readOGR(dsn = paste0(path, "/data_inter/Maps"), layer = "Local_Authority_Districts__December_2018__Boundaries_UK_BFC", 
                  verbose=FALSE)
#shp_LA <- readOGR(dsn = paste0(path, "/Raw_data/Maps"), layer = "Local_Authority_Districts__December_2018__Boundaries_UK_BFC")
shp_UK_LA <- subset(shp_LA, lad18cd %in% Mapping_df$orig)
centroids_LA <- as.data.frame(shp_LA)[,c(2,7,8)]

# merge for spatial units
lad_prediction <- merge(Lad_df, centroids_LA, by.x="orig", by.y="lad18cd")
lad_prediction<- merge(lad_prediction, centroids_LA, by.x="dest", by.y="lad18cd")

lad_prediction <- lad_prediction %>%
  filter(lad_prediction$model.all2009>2800) # remove 75% of bottom flows so it runs


# DF for maps
names(lad_prediction) <- c("dest", "orig", "id_flow", "prediction_2009", "hl", "year", "Origin","Destination", "lng_orig", "lat_orig","lng_dest", "lat_dest")

```

```{r LAD plot}

flows_lad <- gcIntermediate(lad_prediction[,9:10], lad_prediction[,11:12], sp = TRUE, addStartEnd = TRUE)
flows_lad$ID <- lad_prediction$id_flow
flows_lad$prediction <- round(lad_prediction$prediction_2009, 2)
flows_lad$prediction_log <- log(lad_prediction$prediction_2009)
flows_lad$hl <- lad_prediction$hl
flows_lad$origins <- lad_prediction$orig
flows_lad$destinations <- lad_prediction$dest
flows_lad$Origin <- lad_prediction$Origin
flows_lad$Destinaton <- lad_prediction$Destination


# create crosstalk sharedata
lad_map <- SharedData$new(flows_lad)
lad_df <- SharedData$new(as.data.frame(flows_lad@data), group = lad_map $groupName())

pal2 <- colorBin(palette = "plasma", domain=c(6,9), bins = 10, pretty = FALSE)

filter_slider("Prediction", "Â£100k in 2010", lad_df, column=~prediction, step=50, max = 64000, width = 600)

#maps
leaflet(options = leafletOptions(minZoom = 5)) %>%
  addProviderTiles("CartoDB.Positron") %>%
  setMaxBounds( lng1 = -8
                , lat1 = 48
                , lng2 = 2
                , lat2 = 61 ) %>%
  addPolylines(data = lad_map, weight = ~prediction_log/7, color = ~pal2(prediction_log), label = ~ID)
```

Column {data-width=400}
-------------------------------------

```{r LAD sliders, echo=FALSE}
#filters

filter_select("Origin", "Origin", lad_df, ~Origin)
#datatable
datatable(lad_df, extensions="Scroller", style="bootstrap", class="compact",rownames= FALSE,
          width="100%", options=list(deferRender=TRUE, scrollY=300,
                                     scroller=TRUE, columnDefs = list(list(visible=FALSE, targets=c(0,2,4,5)))))
```

<br>

Tranos, E., A. Carrascal-Incera and G. Willis (2022) Using the web to predict regional trade flows: data extraction, modelling, and validation, *Annals of the AAG*. In press. [Link.]()

<small>Only the top *x%* of predicted trade flows are included in the visualisations.</small>
